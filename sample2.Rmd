---
title: "Data Science Sample 2"
author: "Mason Shihab"
date: "3/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(tidycensus)
library(WVPlots)
library(caret)
library(rpart)
library(rpart.plot)
library(ipred)
library(ranger)
library(h2o)
library(xgboost)
library(gbm)
```

# Introduction

Today we will be working with two data sets: census demographic data and results from the 2020 election. We will first join these two data sets together and then partition the combined data between test and train data sets. We will then use the train data set in order to attempt to predict the winner of the election in each county. Accuracy of these predictions will be measured using confusion matrices.

Various data science prediction methods will then be ran and compared against each other for educational and practice purposes with the "accuracy" metric of confusion matrices. We will then decide which model to use when attempting to predict election results based on census data.

## Pre-work

### Loading the data.
```{r}
var <- load_variables(2019, "acs1", cache = TRUE)
```

Here we load data using my API key which is stored in my system.

### Selecting the variables

```{r}
census_data <- get_acs(geography = "county", 
                       variables = c(medage = "B01002_001",
                                     totalpop = "B01003_001",
                                     medincome = "B06011_001",
                                     mexican = "B03001_004",
                                     soc_sci = "B15012_006", 
                                     chinese = "B02015_007", 
                                     phd = "B15003_025",
                                     black = "B02001_003",
                                     canadian = "B05006_167",
                                     female = "B01001_026",
                                     lgbtmarried = "B09019_011",
                                     clean_energy = "B25040_008"),
                                     year = 2019)


census_step = census_data %>% 
  pivot_wider(id_cols = c(NAME), names_from = variable, values_from = estimate) %>% 
  separate(NAME, into = c("county", "state"), sep = ", ") %>% arrange(state, county)


census_wider = census_step %>% 
  mutate(pctmex = mexican/totalpop) %>% mutate(pct_soc_sci = soc_sci/totalpop) %>%  
  mutate(pctchn = chinese/totalpop) %>% 
  mutate(pctphd = phd/totalpop) %>% mutate(pctblack = black/totalpop) %>% 
  mutate(pctcnd = canadian/totalpop) %>% mutate(pctfem = female/totalpop) %>% 
  mutate(pctlgbtm = lgbtmarried/totalpop) %>% mutate(pctsolar = clean_energy/totalpop)

```

I have selected 12 variables which I believe can predict the winner within each county.

totalpop: Total population in that county. This variable was also used to obtain percentages.  

medage: Median county age.  

medincome: Median county income.  

pctmex: Percent of county from Mexico.  

pct_soc_sci: Percent of county who studied social sciences in undergrad.  

pctchn: Percent of county from China.  

pctphd: Percent of county with a doctorate.  

pctblack: Percent of county that is African American.  

pctcnd: Percent of county of Canadian origin.  

pctfem: Percent of county that is female.  

pctlgbtm: Percent of county with a same-sex spouse.  

pctsolar: Percent of county that uses solar panels.  



### Creating the data set.

```{r}
election = read_csv("president_county_candidate.csv", 
    col_types = cols(won = col_skip()))

census_drop_na = drop_na(census_wider)

election2 = election %>% arrange(state, county) %>% 
  group_by(state, county) %>% 
  mutate(pctvote = 100*total_votes/sum(total_votes)) %>% 
  filter(candidate %in% c("Joe Biden", "Donald Trump"))

election_clean = election2 %>% 
  pivot_wider(id_cols = c(state, county), 
              names_from = candidate, values_from = pctvote)

election_clean %>% inner_join(census_drop_na) -> data_clean
```

Here I join election results with my set of census variables.


## Test / Train split

```{r}
set.seed(2)
data_clean = data_clean %>% mutate(winner = 
                        factor(case_when(`Joe Biden` < `Donald Trump` ~ "Trump",
                                  `Joe Biden` > `Donald Trump` ~ "Biden")))

sample_data <- sample.int(n = nrow(data_clean), 
                          size = floor(.8*nrow(data_clean)), replace = F)
train_data <- data_clean[sample_data,]
test_data  <- data_clean[-sample_data,]

```

Here I split this new, combined data set into test / train partitions. 80% of the
data will be used to predict the other 20%. We will then test the accuracy of 
that remaining 20% below.

## Method 1: Decision Tree

```{r}
set.seed(2)
tree <- rpart(winner ~ totalpop + medage + medincome + pctmex + pct_soc_sci + 
                pctchn + pctphd + pctblack + 
                pctcnd + pctfem + pctlgbtm + pctsolar,
                data = train_data, method = "class",
                control = rpart.control(cp = 0.03))

rpart.plot(tree, yesno = 2, type = 1)
```


This is an RPart tree, which automatically selects the variables that are most explanatory.

According to my tree, counties where at least 1.7% of the population majored (while in college, however long ago it was) in social sciences have a 63% chance of voting for Biden. (In this case, "voting for Biden" means Biden winning the county by at least one vote.) This made up 12% of the data. However, in places where less than 1.7% majored in social sciences, the prediction turns on whether or not the proportion of African Americans is above or under 45%. If above, Biden has an 89% chance of winning the county, if under, Trump has a 94% chance. In counties that are at least 1.7% social sciences majors, Trump has a 62% of winning if less than .45% of the population is Chinese. But if a county has more than 1.7% social science majors and is more than half a percent Chinese, then that county had at least an 86% chance of voting for Biden. 

```{r}
confusionMatrix(predict(tree, test_data, type = "class"), test_data$winner)
```


Using this tree, we can predict the remaining data with an accuracy of over 90%.



## Method 2: Bagging

```{r}
set.seed(2)
data_bag <- bagging(
  formula = winner ~ totalpop + medage + medincome + pctmex + pct_soc_sci + 
                pctchn + pctphd + pctblack + 
                pctcnd + pctfem + pctlgbtm + pctsolar,
  data = train_data,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

data_bag

confusionMatrix(predict(data_bag, test_data, type = "class"), test_data$winner)
```

After bagging, (given set.seed(2)), the accuracy improved to over 94% This is because bagging takes advantage of bootstrapping to run through the data many times and averaging to reduce variance. The drawbacks of this however are the greater computational difficulty and less interpretability. Also, due to tree correlation, the models in the tree share common features at their earlier branchings. This reduces model independence, so some of the higher accuracy could come at the cost of some level of generalizability (overfitting).



## Method 3: Random Forests

```{r}
set.seed(2)
n_features <- length(setdiff(names(train_data), "winner"))

data_rf1 <- ranger(
  winner ~
    totalpop + medage + medincome + pctmex + pct_soc_sci + 
                pctchn + pctphd + pctblack + 
                pctcnd + pctfem + pctlgbtm + pctsolar, 
  data = train_data,
  mtry = floor(n_features/3),
  respect.unordered.factors = "order",
  seed = 123
)

data_rf1


data_rf2 <- predict(data_rf1, test_data)
confusionMatrix(data_rf2$predictions, test_data$winner)

```
Random forests provide  higher accuracy, lower computational time (at least in this case), and greater interpretability due to de-correlated trees. This decorrelation stands to improve predictive performance and interpretability by splitting on a limited random selection of variables (or features) from different bootstrapped versions each time a split is created. This is known as split-variable randomization. This method saves some of the lost generalizability from bagging. In this case, it's a win-win because we have a similar accuracy level to bagging.


## Method 4: Gradient Boost

```{r}
set.seed(2)
train_data2 = train_data %>% mutate(Biden_win = (ifelse(winner=="Biden", 1, 0)))

test_data2 = test_data %>% mutate(winner = factor(ifelse(winner=='Biden', 1, 0)))

data_gbm <- gbm(formula = Biden_win ~ totalpop + medage + medincome + 
                  pctmex + pct_soc_sci + 
                  pctchn + pctphd + pctblack + 
                  pctcnd + pctfem + pctlgbtm + pctsolar,
                data = train_data2,
                distribution = "bernoulli",
                n.trees = 500,
                shrinkage = 0.1,
                interaction.depth = 3,
                n.minobsinnode = 10,
                cv.folds = 10
)

data_gbm


model_gbm = as.factor(round(predict(data_gbm, test_data, type = 'response')))
confusionMatrix(model_gbm, test_data2$winner)
```

The gradient boosted model was even more accurate than the random forest, reaching 95%. Gradient boosting involves aggregating many shallow trees into a committee, such that each tree improves on the one before it. This method is particularly good for underfit data, or data with high bias and low variance. Gradiant boosting is often the strongest choice because it corrects itself as new trees are added. (However, there is a higher chance of underfitting than with random forests.)



# Conclusion

I suggest that when attempting to predict election results based on census data to use random forests, as it performed nearly as well as gradiant boosting and likely did so with less overfitting, making it more generalizable.
