<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Mason Shihab" />


<title>Data Science Sample 2</title>

<script src="site_libs/header-attrs-2.5/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/simplex.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 41px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h2 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h3 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h4 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h5 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h6 {
  padding-top: 46px;
  margin-top: -46px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Mason's Github</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Sample1.html">Data Science Sample 1</a>
</li>
<li>
  <a href="Sample2.html">Data Science Sample 2</a>
</li>
<li>
  <a href="Resume.pdf">Resume</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Data Science Sample 2</h1>
<h4 class="author">Mason Shihab</h4>
<h4 class="date">3/28/2021</h4>

</div>


<div id="portfolio-prompts" class="section level1">
<h1>Portfolio Prompts</h1>
<p>Title of Project: Homeworks 5 and 6, Election Prediction</p>
<p>Brief Description of Problem: What variables will most explain the election outcome and what simple machine learning method is most accurate?</p>
<p>What problem were you solving?: See each problem below.</p>
<p>Solution: See each problem below for solution.</p>
<p>How did you solve the problem?: API Keys, Data manipulation, test/train, decision trees and confusion matrices.</p>
<p>What techniques did you use? Why?: Using multiple machine learning algorithms to compare accuracy. This method is best so we can compare accuracy and see which model best fits the data and the assumptions we are making.</p>
<p>Were there any considerations you had to make along the way?: Yes, first which variables to use from the ACS dataset (American Community Survey). In addition, it was difficult to learn how to use the various models used below, as the data need to be in the right format.</p>
<p>Has this project been completed yet?: Yes</p>
<p>Role: Student</p>
<p>Did you complete this work individually or as part of a team?: Individually</p>
<p>Tools / Languages Used: R, RStudio</p>
<p>Tell us about any applicable languages, business intelligence tools, databases, etc. It would also be helpful to know about any interesting libraries that you used where applicable.Relevant Links Any relevant Github repositories, write-ups, or hosted works: GGPlot, dplyr, and tidyverse were used here. In addition, Rpart, caret, ipred, ranger, gbm, and xboost. Also used were publicly available election results, which were joined by ACS data that was downloaded using my API Key. I can provide more details on getting a similar API Key upon request.</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Today we will be working with two data sets: census demographic data and results from the 2020 election. We will first join these two data sets together and then partition the combined data between test and train data sets. We will then use the train data set in order to attempt to predict the winner of the election in each county. Accuracy of these predictions will be measured using confusion matrices.</p>
<p>Various data science prediction methods will then be ran and compared against each other for educational and practice purposes with the “accuracy” metric of confusion matrices. We will then decide which model to use when attempting to predict election results based on census data.</p>
<div id="pre-work" class="section level2">
<h2>Pre-work</h2>
<div id="loading-the-data." class="section level3">
<h3>Loading the data.</h3>
<pre class="r"><code>var &lt;- load_variables(2019, &quot;acs1&quot;, cache = TRUE)</code></pre>
<p>Here we load data using my API key which is stored in my system.</p>
</div>
<div id="selecting-the-variables" class="section level3">
<h3>Selecting the variables</h3>
<pre class="r"><code>census_data &lt;- get_acs(geography = &quot;county&quot;, 
                       variables = c(medage = &quot;B01002_001&quot;,
                                     totalpop = &quot;B01003_001&quot;,
                                     medincome = &quot;B06011_001&quot;,
                                     mexican = &quot;B03001_004&quot;,
                                     soc_sci = &quot;B15012_006&quot;, 
                                     chinese = &quot;B02015_007&quot;, 
                                     phd = &quot;B15003_025&quot;,
                                     black = &quot;B02001_003&quot;,
                                     canadian = &quot;B05006_167&quot;,
                                     female = &quot;B01001_026&quot;,
                                     lgbtmarried = &quot;B09019_011&quot;,
                                     clean_energy = &quot;B25040_008&quot;),
                                     year = 2019)</code></pre>
<pre><code>## Getting data from the 2015-2019 5-year ACS</code></pre>
<pre class="r"><code>census_step = census_data %&gt;% 
  pivot_wider(id_cols = c(NAME), names_from = variable, values_from = estimate) %&gt;% 
  separate(NAME, into = c(&quot;county&quot;, &quot;state&quot;), sep = &quot;, &quot;) %&gt;% arrange(state, county)


census_wider = census_step %&gt;% 
  mutate(pctmex = mexican/totalpop) %&gt;% mutate(pct_soc_sci = soc_sci/totalpop) %&gt;%  
  mutate(pctchn = chinese/totalpop) %&gt;% 
  mutate(pctphd = phd/totalpop) %&gt;% mutate(pctblack = black/totalpop) %&gt;% 
  mutate(pctcnd = canadian/totalpop) %&gt;% mutate(pctfem = female/totalpop) %&gt;% 
  mutate(pctlgbtm = lgbtmarried/totalpop) %&gt;% mutate(pctsolar = clean_energy/totalpop)</code></pre>
<p>I have selected 12 variables which I believe can predict the winner within each county.</p>
<p>totalpop: Total population in that county. This variable was also used to obtain percentages.</p>
<p>medage: Median county age.</p>
<p>medincome: Median county income.</p>
<p>pctmex: Percent of county from Mexico.</p>
<p>pct_soc_sci: Percent of county who studied social sciences in undergrad.</p>
<p>pctchn: Percent of county from China.</p>
<p>pctphd: Percent of county with a doctorate.</p>
<p>pctblack: Percent of county that is African American.</p>
<p>pctcnd: Percent of county of Canadian origin.</p>
<p>pctfem: Percent of county that is female.</p>
<p>pctlgbtm: Percent of county with a same-sex spouse.</p>
<p>pctsolar: Percent of county that uses solar panels.</p>
</div>
<div id="creating-the-data-set." class="section level3">
<h3>Creating the data set.</h3>
<pre class="r"><code>election = read_csv(&quot;president_county_candidate.csv&quot;, 
    col_types = cols(won = col_skip()))

census_drop_na = drop_na(census_wider)

election2 = election %&gt;% arrange(state, county) %&gt;% 
  group_by(state, county) %&gt;% 
  mutate(pctvote = 100*total_votes/sum(total_votes)) %&gt;% 
  filter(candidate %in% c(&quot;Joe Biden&quot;, &quot;Donald Trump&quot;))

election_clean = election2 %&gt;% 
  pivot_wider(id_cols = c(state, county), 
              names_from = candidate, values_from = pctvote)

election_clean %&gt;% inner_join(census_drop_na) -&gt; data_clean</code></pre>
<pre><code>## Joining, by = c(&quot;state&quot;, &quot;county&quot;)</code></pre>
<p>Here I join election results with my set of census variables.</p>
</div>
</div>
<div id="test-train-split" class="section level2">
<h2>Test / Train split</h2>
<pre class="r"><code>set.seed(2)
data_clean = data_clean %&gt;% mutate(winner = 
                        factor(case_when(`Joe Biden` &lt; `Donald Trump` ~ &quot;Trump&quot;,
                                  `Joe Biden` &gt; `Donald Trump` ~ &quot;Biden&quot;)))

sample_data &lt;- sample.int(n = nrow(data_clean), 
                          size = floor(.8*nrow(data_clean)), replace = F)
train_data &lt;- data_clean[sample_data,]
test_data  &lt;- data_clean[-sample_data,]</code></pre>
<p>Here I split this new, combined data set into test / train partitions. 80% of the data will be used to predict the other 20%. We will then test the accuracy of that remaining 20% below.</p>
</div>
<div id="method-1-decision-tree" class="section level2">
<h2>Method 1: Decision Tree</h2>
<pre class="r"><code>set.seed(2)
tree &lt;- rpart(winner ~ totalpop + medage + medincome + pctmex + pct_soc_sci + 
                pctchn + pctphd + pctblack + 
                pctcnd + pctfem + pctlgbtm + pctsolar,
                data = train_data, method = &quot;class&quot;,
                control = rpart.control(cp = 0.03))

rpart.plot(tree, yesno = 2, type = 1)</code></pre>
<p><img src="sample2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>This is an RPart tree, which automatically selects the variables that are most explanatory.</p>
<p>According to my tree, counties where at least 1.7% of the population majored (while in college, however long ago it was) in social sciences have a 63% chance of voting for Biden. (In this case, “voting for Biden” means Biden winning the county by at least one vote.) This made up 12% of the data. However, in places where less than 1.7% majored in social sciences, the prediction turns on whether or not the proportion of African Americans is above or under 45%. If above, Biden has an 89% chance of winning the county, if under, Trump has a 94% chance. In counties that are at least 1.7% social sciences majors, Trump has a 62% of winning if less than .45% of the population is Chinese. But if a county has more than 1.7% social science majors and is more than half a percent Chinese, then that county had at least an 86% chance of voting for Biden.</p>
<pre class="r"><code>confusionMatrix(predict(tree, test_data, type = &quot;class&quot;), test_data$winner)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Trump Biden
##      Trump   486    38
##      Biden    19    66
##                                           
##                Accuracy : 0.9064          
##                  95% CI : (0.8804, 0.9283)
##     No Information Rate : 0.8292          
##     P-Value [Acc &gt; NIR] : 3.889e-08       
##                                           
##                   Kappa : 0.6437          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.01712         
##                                           
##             Sensitivity : 0.9624          
##             Specificity : 0.6346          
##          Pos Pred Value : 0.9275          
##          Neg Pred Value : 0.7765          
##              Prevalence : 0.8292          
##          Detection Rate : 0.7980          
##    Detection Prevalence : 0.8604          
##       Balanced Accuracy : 0.7985          
##                                           
##        &#39;Positive&#39; Class : Trump           
## </code></pre>
<p>Using this tree, we can predict the remaining data with an accuracy of over 90%.</p>
</div>
<div id="method-2-bagging" class="section level2">
<h2>Method 2: Bagging</h2>
<pre class="r"><code>set.seed(2)
data_bag &lt;- bagging(
  formula = winner ~ totalpop + medage + medincome + pctmex + pct_soc_sci + 
                pctchn + pctphd + pctblack + 
                pctcnd + pctfem + pctlgbtm + pctsolar,
  data = train_data,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

data_bag</code></pre>
<pre><code>## 
## Bagging classification trees with 100 bootstrap replications 
## 
## Call: bagging.data.frame(formula = winner ~ totalpop + medage + medincome + 
##     pctmex + pct_soc_sci + pctchn + pctphd + pctblack + pctcnd + 
##     pctfem + pctlgbtm + pctsolar, data = train_data, nbagg = 100, 
##     coob = TRUE, control = rpart.control(minsplit = 2, cp = 0))
## 
## Out-of-bag estimate of misclassification error:  0.0731</code></pre>
<pre class="r"><code>confusionMatrix(predict(data_bag, test_data, type = &quot;class&quot;), test_data$winner)</code></pre>
<pre><code>## Warning in confusionMatrix.default(predict(data_bag, test_data, type = &quot;class&quot;), : Levels are not in the same order for reference and
## data. Refactoring data to match.</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Trump Biden
##      Trump   485    15
##      Biden    20    89
##                                          
##                Accuracy : 0.9425         
##                  95% CI : (0.921, 0.9596)
##     No Information Rate : 0.8292         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.8009         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.499          
##                                          
##             Sensitivity : 0.9604         
##             Specificity : 0.8558         
##          Pos Pred Value : 0.9700         
##          Neg Pred Value : 0.8165         
##              Prevalence : 0.8292         
##          Detection Rate : 0.7964         
##    Detection Prevalence : 0.8210         
##       Balanced Accuracy : 0.9081         
##                                          
##        &#39;Positive&#39; Class : Trump          
## </code></pre>
<p>After bagging, (given set.seed(2)), the accuracy improved to over 94% This is because bagging takes advantage of bootstrapping to run through the data many times and averaging to reduce variance. The drawbacks of this however are the greater computational difficulty and less interpretability. Also, due to tree correlation, the models in the tree share common features at their earlier branchings. This reduces model independence, so some of the higher accuracy could come at the cost of some level of generalizability (overfitting).</p>
</div>
<div id="method-3-random-forests" class="section level2">
<h2>Method 3: Random Forests</h2>
<pre class="r"><code>set.seed(2)
n_features &lt;- length(setdiff(names(train_data), &quot;winner&quot;))

data_rf1 &lt;- ranger(
  winner ~
    totalpop + medage + medincome + pctmex + pct_soc_sci + 
                pctchn + pctphd + pctblack + 
                pctcnd + pctfem + pctlgbtm + pctsolar, 
  data = train_data,
  mtry = floor(n_features/3),
  respect.unordered.factors = &quot;order&quot;,
  seed = 123
)

data_rf1</code></pre>
<pre><code>## Ranger result
## 
## Call:
##  ranger(winner ~ totalpop + medage + medincome + pctmex + pct_soc_sci +      pctchn + pctphd + pctblack + pctcnd + pctfem + pctlgbtm +      pctsolar, data = train_data, mtry = floor(n_features/3),      respect.unordered.factors = &quot;order&quot;, seed = 123) 
## 
## Type:                             Classification 
## Number of trees:                  500 
## Sample size:                      2435 
## Number of independent variables:  12 
## Mtry:                             8 
## Target node size:                 1 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error:             7.15 %</code></pre>
<pre class="r"><code>data_rf2 &lt;- predict(data_rf1, test_data)
confusionMatrix(data_rf2$predictions, test_data$winner)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Trump Biden
##      Trump   489    17
##      Biden    16    87
##                                           
##                Accuracy : 0.9458          
##                  95% CI : (0.9247, 0.9624)
##     No Information Rate : 0.8292          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.8079          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9683          
##             Specificity : 0.8365          
##          Pos Pred Value : 0.9664          
##          Neg Pred Value : 0.8447          
##              Prevalence : 0.8292          
##          Detection Rate : 0.8030          
##    Detection Prevalence : 0.8309          
##       Balanced Accuracy : 0.9024          
##                                           
##        &#39;Positive&#39; Class : Trump           
## </code></pre>
<p>Random forests provide higher accuracy, lower computational time (at least in this case), and greater interpretability due to de-correlated trees. This decorrelation stands to improve predictive performance and interpretability by splitting on a limited random selection of variables (or features) from different bootstrapped versions each time a split is created. This is known as split-variable randomization. This method saves some of the lost generalizability from bagging. In this case, it’s a win-win because we have a similar accuracy level to bagging.</p>
</div>
<div id="method-4-gradient-boost" class="section level2">
<h2>Method 4: Gradient Boost</h2>
<pre class="r"><code>set.seed(2)
train_data2 = train_data %&gt;% mutate(Biden_win = (ifelse(winner==&quot;Biden&quot;, 1, 0)))

test_data2 = test_data %&gt;% mutate(winner = factor(ifelse(winner==&#39;Biden&#39;, 1, 0)))

data_gbm &lt;- gbm(formula = Biden_win ~ totalpop + medage + medincome + 
                  pctmex + pct_soc_sci + 
                  pctchn + pctphd + pctblack + 
                  pctcnd + pctfem + pctlgbtm + pctsolar,
                data = train_data2,
                distribution = &quot;bernoulli&quot;,
                n.trees = 500,
                shrinkage = 0.1,
                interaction.depth = 3,
                n.minobsinnode = 10,
                cv.folds = 10
)

data_gbm</code></pre>
<pre><code>## gbm(formula = Biden_win ~ totalpop + medage + medincome + pctmex + 
##     pct_soc_sci + pctchn + pctphd + pctblack + pctcnd + pctfem + 
##     pctlgbtm + pctsolar, distribution = &quot;bernoulli&quot;, data = train_data2, 
##     n.trees = 500, interaction.depth = 3, n.minobsinnode = 10, 
##     shrinkage = 0.1, cv.folds = 10)
## A gradient boosted model with bernoulli loss function.
## 500 iterations were performed.
## The best cross-validation iteration was 151.
## There were 12 predictors of which 12 had non-zero influence.</code></pre>
<pre class="r"><code>model_gbm = as.factor(round(predict(data_gbm, test_data, type = &#39;response&#39;)))</code></pre>
<pre><code>## Using 151 trees...</code></pre>
<pre class="r"><code>confusionMatrix(model_gbm, test_data2$winner)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 489  14
##          1  16  90
##                                           
##                Accuracy : 0.9507          
##                  95% CI : (0.9304, 0.9665)
##     No Information Rate : 0.8292          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.8274          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.8551          
##                                           
##             Sensitivity : 0.9683          
##             Specificity : 0.8654          
##          Pos Pred Value : 0.9722          
##          Neg Pred Value : 0.8491          
##              Prevalence : 0.8292          
##          Detection Rate : 0.8030          
##    Detection Prevalence : 0.8259          
##       Balanced Accuracy : 0.9169          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>The gradient boosted model was even more accurate than the random forest, reaching 95%. Gradient boosting involves aggregating many shallow trees into a committee, such that each tree improves on the one before it. This method is particularly good for underfit data, or data with high bias and low variance. Gradiant boosting is often the strongest choice because it corrects itself as new trees are added. (However, there is a higher chance of underfitting than with random forests.)</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>I suggest that when attempting to predict election results based on census data to use random forests, as it performed nearly as well as gradiant boosting and likely did so with less overfitting, making it more generalizable.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
